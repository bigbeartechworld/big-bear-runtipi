{
  "name": "Ollama - CPU",
  "id": "ollama-cpu",
  "available": true,
  "short_desc": "LLMs inference server with OpenAI compatible API",
  "author": "ollama",
  "port": 11434,
  "categories": ["BigBearCasaOS"],
  "description": "Get up and running with Llama 3, Mistral, Gemma, and other large language models.",
  "tipi_version": 1,
  "version": "0.12.6",
  "supported_architectures": ["arm64", "amd64"]
}
