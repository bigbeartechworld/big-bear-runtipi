{
  "name": "Ollama - AMD",
  "available": true,
  "port": 11434,
  "exposable": true,
  "dynamic_config": true,
  "id": "ollama-amd",
  "description": "Get up and running with Llama 3, Mistral, Gemma, and other large language models.",
  "tipi_version": 1,
  "version": "0.13.5-rocm",
  "categories": [
    "utilities"
  ],
  "short_desc": "LLMs inference server with OpenAI compatible API",
  "author": "ollama",
  "source": "",
  "website": "https://hub.docker.com/r/ollama/ollama",
  "supported_architectures": [
    "amd64",
    "arm64"
  ],
  "created_at": 1768353963000,
  "updated_at": 1768353963000,
  "$schema": "../app-info-schema.json",
  "min_tipi_version": "4.5.0"
}
